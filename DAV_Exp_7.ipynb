{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BajajF/DAV/blob/main/DAV_Exp_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVs1ls4UhvU"
      },
      "source": [
        "**Aim:**<br>\n",
        "Getting introduced to visualization libraries in Python and R. <br>\n",
        "\n",
        "\n",
        "**Lab Objectives:**<br> To effectively use libraries for data analytics.<br>\n",
        "\n",
        "**Lab Outcomes:**<br>\n",
        "Implement visualization techniques to given data sets using R\n",
        "Implement visualization techniques to given data sets using Python\n",
        "\n",
        "\n",
        "\n",
        "**Lab Objectives:**<br>\n",
        "To effectively use libraries for data analytics.<br>\n",
        "Identify 5 Python and R Libraries for Data Visualization.<br>\n",
        "Prepare a brief summary about their features and applications. <br>\n",
        "Perform simple experiments on 2 Libraries each for Python & R\n",
        "\n",
        "**Theory :**\n",
        "\n",
        "**Python Text Analytics Libraries:**\n",
        "\n",
        "**1) NLTK (Natural Language Toolkit):**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Tokenization, stemming, lemmatization.\n",
        "\n",
        "b) Part-of-speech tagging, named entity recognition.\n",
        "\n",
        "c) Concordance and collocation analysis.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Sentiment analysis, text classification.\n",
        "\n",
        "b)Information retrieval, language modeling.\n",
        "\n",
        "**2) spaCy:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Tokenization, POS tagging, and named entity recognition.\n",
        "\n",
        "b) Dependency parsing, sentence segmentation.\n",
        "\n",
        "c) Pre-trained models for multiple languages.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Named entity recognition, information extraction.\n",
        "\n",
        "b) Natural language understanding in chatbots.\n",
        "\n",
        "**3) TextBlob:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Simple API for common NLP tasks.\n",
        "\n",
        "b) Sentiment analysis, noun phrase extraction.\n",
        "\n",
        "c) Part-of-speech tagging.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Sentiment analysis of customer reviews.\n",
        "\n",
        "b) Basic text processing for beginners.\n",
        "\n",
        "**4) Gensim:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Topic modeling (e.g., LDA).\n",
        "\n",
        "b) Document similarity analysis.\n",
        "\n",
        "c) Word embedding models (Word2Vec, Doc2Vec).\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Topic modeling in large document collections.\n",
        "\n",
        "b) Document similarity and clustering.\n",
        "\n",
        "**5) Transformers (Hugging Face):**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) State-of-the-art pre-trained models (e.g., BERT, GPT).\n",
        "\n",
        "b) Easy integration for various NLP tasks.\n",
        "\n",
        "c) Fine-tuning capabilities.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Named entity recognition, sentiment analysis.\n",
        "\n",
        "b) Text generation, language translation.\n",
        "\n",
        "**R Text Analytics Libraries:**\n",
        "\n",
        "**1) tm (Text Mining Package):**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Text preprocessing: Cleaning, stemming, stopword removal.\n",
        "\n",
        "b) Document-term matrix creation.\n",
        "\n",
        "c) Basic text mining functions.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Topic modeling, clustering.\n",
        "\n",
        "b) Sentiment analysis, document classification.\n",
        "\n",
        "**2) quanteda:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Fast and flexible text processing.\n",
        "\n",
        "b) Tokenization, n-grams, and corpus analysis.\n",
        "\n",
        "c) Support for advanced text analysis functions.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Content analysis, sentiment analysis.\n",
        "\n",
        "b) Document-feature matrix creation.\n",
        "\n",
        "**3) tm.plugin.sentiment:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Sentiment analysis using pre-trained models.\n",
        "\n",
        "b) Integration with the tm package.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Analyzing sentiment in textual data.\n",
        "\n",
        "**4) textTinyR:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Efficient text classification.\n",
        "\n",
        "b) Lightweight implementation for quick analysis.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Fast text classification tasks.\n",
        "\n",
        "**5) quanteda.textstats:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Basic text statistics.\n",
        "\n",
        "b) Word frequency analysis, readability scores.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Exploratory text analysis.\n",
        "\n",
        "b) Extracting insights from text data.\n",
        "\n",
        "**Steps Involved in Text Analytics:**\n",
        "\n",
        "**1) Data Collection:** Gather text data from various sources such as documents, social media, or websites.\n",
        "\n",
        "**2) Text Preprocessing:** Clean and preprocess the text data by removing noise, handling missing values, and performing tasks like tokenization and stemming.\n",
        "\n",
        "**3) Text Exploration:** Explore the data through descriptive statistics, word frequency analysis, and visualization.\n",
        "\n",
        "**4) Feature Extraction:** Convert the text into a numerical format, often using techniques like TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "**5)Model Building:** Apply machine learning or natural language processing models for tasks such as sentiment analysis, classification, or clustering.\n",
        "\n",
        "**6) Evaluation:** Assess the performance of the models using appropriate metrics based on the task.\n",
        "\n",
        "**7) Visualization:** Visualize the results to gain insights and interpret the findings.\n",
        "\n",
        "**Benefits of Text Analytics:**\n",
        "\n",
        "**1) Insight Generation:** Extract valuable insights from large volumes of unstructured text data.\n",
        "\n",
        "**2) Decision Support:** Support decision-making processes by providing data-driven insights.\n",
        "\n",
        "**3) Automation:** Automate the analysis of textual data, saving time and resources.\n",
        "\n",
        "**4) Customer Feedback Analysis:** Understand customer sentiments and opinions from reviews and feedback.\n",
        "\n",
        "**5) Information Retrieval:** Enhance search functionalities by improving the relevance of retrieved information.\n",
        "\n",
        "**6) Fraud Detection:** Identify patterns or anomalies in textual data that may indicate fraudulent activities.\n",
        "\n",
        "**7) Competitive Intelligence:** Monitor and analyze competitor activities and market trends from textual sources.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64gvyx4WBbG"
      },
      "source": [
        "# **Text Analytics in Python**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hM27nyWfuB6",
        "outputId": "743c3374-1a92-4e0e-a2b2-3255504dc17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1wvAaU8f6J7",
        "outputId": "5bfebfe0-ec72-4d2b-9ecb-b80614740fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXKbAYZpUjTv",
        "outputId": "86c832de-06a0-4491-8d07-b98fe5eac674"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0D3InJ0gOg3"
      },
      "outputs": [],
      "source": [
        "# Function to scrape text from a website\n",
        "def scrape_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Extract text from paragraphs\n",
        "    paragraphs = soup.find_all('p')\n",
        "    text = ' '.join([para.get_text() for para in paragraphs])\n",
        "    return text\n",
        "\n",
        "# URL of the Wikipedia article to scrape\n",
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "text = scrape_text(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNP5NyEQf5I3",
        "outputId": "01739e5f-e8b4-4f58-ab6e-34592c8e0527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Distribution:\n",
            "[(',', 74), ('the', 61), ('of', 55), ('.', 44), ('and', 31), ('[', 29), (']', 29), ('in', 26), ('language', 22), ('to', 19)]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (Sentence & Word)\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Frequency Distribution\n",
        "fdist = FreqDist(words)\n",
        "print(\"Frequency Distribution:\")\n",
        "print(fdist.most_common(10))\n",
        "\n",
        "# Remove stopwords & punctuations\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "porter = PorterStemmer()\n",
        "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Part of Speech tagging\n",
        "pos_tags = pos_tag(filtered_words)\n",
        "\n",
        "# Named Entity Recognition\n",
        "named_entities = ne_chunk(pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix4Tmi70UjWy",
        "outputId": "e50c2b31-c2aa-48ca-8b0a-7821f7d0d9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed Text:\n",
            "Sentences: 44\n",
            "Words before processing: 1363\n",
            "Words after processing: 686\n",
            "Stemmed Words: ['natur', 'languag', 'process', 'nlp', 'interdisciplinari', 'subfield', 'comput', 'scienc', 'linguist', 'primarili']\n",
            "Lemmatized Words: ['natural', 'language', 'processing', 'nlp', 'interdisciplinary', 'subfield', 'computer', 'science', 'linguistics', 'primarily']\n",
            "POS Tags: [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('interdisciplinary', 'JJ'), ('subfield', 'NN'), ('computer', 'NN'), ('science', 'NN'), ('linguistics', 'NNS'), ('primarily', 'RB')]\n",
            "Named Entities: [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('interdisciplinary', 'JJ'), ('subfield', 'NN'), ('computer', 'NN'), ('science', 'NN'), ('linguistics', 'NNS'), ('primarily', 'RB')]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nProcessed Text:\")\n",
        "print(\"Sentences:\", len(sentences))\n",
        "print(\"Words before processing:\", len(words))\n",
        "print(\"Words after processing:\", len(filtered_words))\n",
        "print(\"Stemmed Words:\", stemmed_words[:10])\n",
        "print(\"Lemmatized Words:\", lemmatized_words[:10])\n",
        "print(\"POS Tags:\", pos_tags[:10])\n",
        "print(\"Named Entities:\", named_entities[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJbhvsquUiCy"
      },
      "source": [
        "# **Text Analytics in R**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tau80TWk5Ah",
        "outputId": "95b4f82e-5d8a-41ba-f1b5-0284a12b9ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages('tokenizers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsB40zKPnd5k",
        "outputId": "f3c835c3-70d1-4131-e337-1c850054b705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘slam’, ‘BH’\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages('tm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages('rvest')"
      ],
      "metadata": {
        "id": "q7tUVuIdwmaL",
        "outputId": "97705d63-768f-456f-b300-c0b2733e6adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tm)\n",
        "library(rvest)\n",
        "library(NLP)\n",
        "library(tokenizers)\n",
        "library(SnowballC)"
      ],
      "metadata": {
        "id": "bfGtzWcjwIgZ",
        "outputId": "24724ba7-d44f-4cc0-f200-b8043634b934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: NLP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF7pfOclfPxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268f7764-1c67-481a-e558-582127236948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: He raced to the grocery store. He went inside but realized he forgot his wallet. He raced back home to grab it. Once he found it, he raced to the car again and drove back to the grocery store. \n",
            "Word Tokens: he raced to the grocery store he went inside but realized he forgot his wallet he raced back home to grab it once he found it he raced to the car again and drove back to the grocery store \n"
          ]
        }
      ],
      "source": [
        "text <- \"He raced to the grocery store. He went inside but realized he forgot his wallet. He raced back home to grab it. Once he found it, he raced to the car again and drove back to the grocery store.\"\n",
        "# Tokenisation\n",
        "nsent_tokens <- unlist(tokenize_sentences(text))\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "cat(\"Sentence Tokens:\", sent_tokens, \"\\n\")\n",
        "cat(\"Word Tokens:\", word_tokens, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "fdist <- table(unlist(word_tokens))\n",
        "print(head(sort(fdist, decreasing = TRUE), 2))"
      ],
      "metadata": {
        "id": "iorvqdzYx2hy",
        "outputId": "675561f3-fb12-4ccc-f591-4f95d26317e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "he to \n",
            " 6  4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and punctuations\n",
        "stop_words <- stopwords(\"en\")\n",
        "filtered_tokens <- word_tokens[!(word_tokens %in% stop_words) & grepl(\"[a-zA-Z]\", word_tokens)]\n",
        "cat(\"Filtered Tokens (without stopwords and punctuations):\", filtered_tokens, \"\\n\")"
      ],
      "metadata": {
        "id": "4rgzwpZFxnuA",
        "outputId": "64d21397-ef39-43a2-ae63-439a736bacf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (without stopwords and punctuations): raced grocery store went inside realized forgot wallet raced back home grab found raced car drove back grocery store \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmed_tokens <- wordStem(filtered_tokens, language = \"en\")\n",
        "cat(\"Stemmed Tokens:\", stemmed_tokens, \"\\n\")"
      ],
      "metadata": {
        "id": "Tv_TfbSXxnx8",
        "outputId": "91e51bf0-56d0-4bc2-deab-921c0d3a629a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: race groceri store went insid realiz forgot wallet race back home grab found race car drove back groceri store \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatized_text <- tolower(text)\n",
        "lemmatized_text <- wordStem(lemmatized_text, language = \"en\")\n",
        "\n",
        "cat(\"Lemmatized Text:\", lemmatized_text, \"\\n\")"
      ],
      "metadata": {
        "id": "EnJSMSlLxn1O",
        "outputId": "45955c6e-dc30-4137-a398-c5a94ce446dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Text: he raced to the grocery store. he went inside but realized he forgot his wallet. he raced back home to grab it. once he found it, he raced to the car again and drove back to the grocery store. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Web Scraping\n",
        "url <- 'http://quotes.toscrape.com/'\n",
        "web_page <- read_html(url)\n",
        "web_text <- html_text(web_page)\n",
        "cat(\"Scraped Data from the Website:\\n\", web_text)"
      ],
      "metadata": {
        "id": "2m7TZRrqyE_C",
        "outputId": "c53c73e7-023d-454c-af10-250abda27633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Data from the Website:\n",
            " Quotes to Scrape\n",
            "    \n",
            "        \n",
            "            \n",
            "                \n",
            "                    Quotes to Scrape\n",
            "                \n",
            "            \n",
            "            \n",
            "                \n",
            "                \n",
            "                    Login\n",
            "                \n",
            "                \n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "\n",
            "    \n",
            "\n",
            "    \n",
            "        “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
            "        by Albert Einstein\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            change\n",
            "            \n",
            "            deep-thoughts\n",
            "            \n",
            "            thinking\n",
            "            \n",
            "            world\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
            "        by J.K. Rowling\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            abilities\n",
            "            \n",
            "            choices\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
            "        by Albert Einstein\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            inspirational\n",
            "            \n",
            "            life\n",
            "            \n",
            "            live\n",
            "            \n",
            "            miracle\n",
            "            \n",
            "            miracles\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "        by Jane Austen\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            aliteracy\n",
            "            \n",
            "            books\n",
            "            \n",
            "            classic\n",
            "            \n",
            "            humor\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
            "        by Marilyn Monroe\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            be-yourself\n",
            "            \n",
            "            inspirational\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “Try not to become a man of success. Rather become a man of value.”\n",
            "        by Albert Einstein\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            adulthood\n",
            "            \n",
            "            success\n",
            "            \n",
            "            value\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “It is better to be hated for what you are than to be loved for what you are not.”\n",
            "        by André Gide\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            life\n",
            "            \n",
            "            love\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “I have not failed. I've just found 10,000 ways that won't work.”\n",
            "        by Thomas A. Edison\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            edison\n",
            "            \n",
            "            failure\n",
            "            \n",
            "            inspirational\n",
            "            \n",
            "            paraphrased\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
            "        by Eleanor Roosevelt\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            misattributed-eleanor-roosevelt\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “A day without sunshine is like, you know, night.”\n",
            "        by Steve Martin\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            humor\n",
            "            \n",
            "            obvious\n",
            "            \n",
            "            simile\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "                Next →\n",
            "            \n",
            "            \n",
            "        \n",
            "    \n",
            "        \n",
            "            Top Ten tags\n",
            "            \n",
            "            \n",
            "            love\n",
            "            \n",
            "            \n",
            "            \n",
            "            inspirational\n",
            "            \n",
            "            \n",
            "            \n",
            "            life\n",
            "            \n",
            "            \n",
            "            \n",
            "            humor\n",
            "            \n",
            "            \n",
            "            \n",
            "            books\n",
            "            \n",
            "            \n",
            "            \n",
            "            reading\n",
            "            \n",
            "            \n",
            "            \n",
            "            friendship\n",
            "            \n",
            "            \n",
            "            \n",
            "            friends\n",
            "            \n",
            "            \n",
            "            \n",
            "            truth\n",
            "            \n",
            "            \n",
            "            \n",
            "            simile\n",
            "            \n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "\n",
            "    \n",
            "    \n",
            "            \n",
            "                Quotes by: GoodReads.com\n",
            "            \n",
            "            \n",
            "                Made with ❤ by Zyte\n",
            "            \n",
            "        \n",
            "    "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :** Text analytics is a powerful tool for extracting valuable information from text data, and the choice of libraries and techniques depends on the specific goals and characteristics of the data at hand."
      ],
      "metadata": {
        "id": "eshO6_nO1J-e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}